# Second Project
 ## Topics of my Second **Deep-Learning** course project:
* In this project I have implemented a `Segnet Network` in order to perform the `Image Classification` task on `UTKFace` dataset.
    
* In **Part A** a **segnet** architecture has been implemented; however, the training scheme and hyperparameters are not the same as the reference papaer: In contrast to the reference paper, I have trained the whole network at once (instead of training different encoder-decoder pairs in seperate steps). Moreover I have utilized two optimizer, `SGD` and `Adam`, to train the net and to compare the results. As mentioned in a paper, a smart person can figure out that **Adam** has a higher convergance rate than **SGD** optimzer, which results in a better performance in lower iterations. The codes and results are provided in [Deep_Segnet_Adam.ipynb](https://github.com/ARokni/Deep-Learning/blob/main/Project%202/PartA/Deep_Segnet_Adam.ipynb) (**Segnet** with **Adam optimizer**) and in [Deep_Segnet_SGD.ipynb](https://github.com/ARokni/Deep-Learning/blob/main/Project%202/PartA/Deep_Segnet_SGD.ipynb) (Segnet with **SGD** optimizer) notebooks.

* In **Part B** the architecture has been implemented in **Part A** has been enriched with Batch-Normalization (BN)layers. On the groud of the paper, I expected that BN layers improve the convergance rate and the optimum point of the network; By simulating the Segnet architecture with BN layers, I found that results are consistent with the reference paper: In lower iterations the network reached better optimum points. An interested person can find the improvement of BN with convergence rate and the performance of the network by comparing the results of the **Part A** and **Part B**. The codes and results of this part are provided in [Deep_Segnet_BN_Adam.ipynb](https://github.com/ARokni/Deep-Learning/blob/main/Project%202/PartB/Deep_Hw2_000214_PartB_Res_Final2_1635_Adam_Decay0_%20OnlyForMe.ipynb).